{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faac2b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:12:29.794728Z",
     "start_time": "2025-08-08T15:12:29.780574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Tool Setup\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import pathlib, os, sys, operator, re, datetime\n",
    "\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import Model\n",
    "import tensorflow_datasets as tfds\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Use seaborn for pairplot.\n",
    "# pip install -q seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "%reload_ext tensorboard\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020f350",
   "metadata": {},
   "source": [
    "## Creating Numpy Array of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd65d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:15:10.094927Z",
     "start_time": "2025-08-08T15:15:10.011661Z"
    }
   },
   "outputs": [],
   "source": [
    "data = 'p:\\Mitchell\\FFNeuralNetwork\\statsTraining.csv'\n",
    "dataAns = 'p:\\Mitchell\\FFNeuralNetwork\\statsAnswer.csv'\n",
    "\n",
    "Smalldata = 'p:\\Mitchell\\FFNeuralNetwork\\SmallBatchTraining.csv'\n",
    "SmalldataAns = 'p:\\Mitchell\\FFNeuralNetwork\\SmallBatchTrainingAns.csv'\n",
    "\n",
    "raw_dataset = pd.read_csv(data, na_values='?', comment='\\t', skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "# dataset.tail()\n",
    "\n",
    "dataset.isna().sum()                                                    # Counts number of Nan Values\n",
    "dataset = dataset.dropna(axis = 0, subset = 'Previous Rank')            # Removes Nan Values from Previous Rank Rows\n",
    "dataset = dataset.fillna(0)                                             # Fills remaining Nan Values with 0\n",
    "testSet = dataset.copy()                                                # Copying dataset to testset\n",
    "dataset = dataset.drop('Next Season Rank', axis = 1)                    # Remove Rank column from training set\n",
    "y = testSet[['Next Season Rank']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3652b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# regular = True \n",
    "\n",
    "# if(regular):\n",
    "#     df = pd.read_csv(data)\n",
    "#     df_y = pd.read_csv(dataAns)\n",
    "\n",
    "#     numpy_array = df.to_numpy()\n",
    "#     numpy_array = numpy_array[:, np.newaxis, :]\n",
    "#     numpy_size = numpy_array.size\n",
    "\n",
    "#     numpy_array_two = df_y.to_numpy()\n",
    "#     numpy_array_two = numpy_array_two.astype(np.float32)\n",
    "\n",
    "#     numpy_array[np.isnan(numpy_array)] = 0\n",
    "#     numpy_array_test = numpy_array.astype(np.float32)\n",
    "\n",
    "#     # print(numpy_array)\n",
    "#     # print(numpy_size)\n",
    "#     # print(numpy_array.shape)\n",
    "#     # print(numpy_array_two.shape)\n",
    "# else:\n",
    "#     df = pd.read_csv(Smalldata)\n",
    "#     df_y = pd.read_csv(SmalldataAns)\n",
    "\n",
    "#     numpy_array = df.to_numpy()\n",
    "#     numpy_array = numpy_array[:, np.newaxis, :]\n",
    "#     numpy_size = numpy_array.size\n",
    "\n",
    "#     numpy_array_two = df_y.to_numpy()\n",
    "#     numpy_array_two = numpy_array_two.astype(np.float32)\n",
    "\n",
    "#     numpy_array[np.isnan(numpy_array)] = 0\n",
    "#     numpy_array_test = numpy_array.astype(np.float32)\n",
    "\n",
    "#     # print(numpy_array)\n",
    "#     # print(numpy_size)\n",
    "#     # print(numpy_array.shape)\n",
    "#     # print(numpy_array_two.shape) \n",
    "\n",
    "# H = numpy_array.shape[0]\n",
    "# W = numpy_array.shape[1]\n",
    "\n",
    "# H2 = numpy_array_two.shape[0]\n",
    "# W2 = numpy_array_two.shape[1]\n",
    "\n",
    "# numpy_array = numpy_array/100\n",
    "# numpy_array_two = numpy_array_two\n",
    "\n",
    "# print(numpy_array.shape)\n",
    "# # print(numpy_array)   \n",
    "# print(numpy_array_two)   \n",
    "\n",
    "# # print(H)\n",
    "# # print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52acdb0b",
   "metadata": {},
   "source": [
    "## Creating Numpy Array of Tesing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5096ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testdata = 'p:\\Mitchell\\FFNeuralNetwork\\statsTesting.csv'\n",
    "# TestdataAns = 'p:\\Mitchell\\FFNeuralNetwork\\statsTestAnswer.csv'\n",
    "\n",
    "# SmallTestdata = 'p:\\Mitchell\\FFNeuralNetwork\\SmallBatchTest.csv'\n",
    "# SmallTestdataAns = 'p:\\Mitchell\\FFNeuralNetwork\\SmallBatchTestAns.csv'\n",
    "\n",
    "# regular = True \n",
    "\n",
    "# if(regular):\n",
    "#     Testdf = pd.read_csv(Testdata)\n",
    "#     Testdf_y = pd.read_csv(TestdataAns)\n",
    "\n",
    "#     numpy_array_test = Testdf.to_numpy()\n",
    "#     numpy_array_test = numpy_array_test[:, np.newaxis, :]\n",
    "#     numpy_sizeT = numpy_array_test.size\n",
    "\n",
    "#     numpy_array_ans = Testdf_y.to_numpy()\n",
    "#     numpy_array_ans = numpy_array_ans.astype(np.float32)\n",
    "\n",
    "#     numpy_array_test[np.isnan(numpy_array_test)] = 0\n",
    "#     numpy_array_test = numpy_array_test.astype(np.float32)\n",
    "# else:\n",
    "#     Testdf = pd.read_csv(SmallTestdata)\n",
    "#     Testdf_y = pd.read_csv(SmallTestdataAns)\n",
    "\n",
    "#     numpy_array_test = Testdf.to_numpy()\n",
    "#     numpy_array_test = numpy_array_test[:, np.newaxis, :]\n",
    "#     numpy_sizeT = numpy_array_test.size\n",
    "\n",
    "#     numpy_array_ans = Testdf_y.to_numpy()\n",
    "#     numpy_array_ans = numpy_array_ans.astype(np.float32)\n",
    "\n",
    "#     numpy_array_test[np.isnan(numpy_array_test)] = 0\n",
    "#     numpy_array_test = numpy_array_test.astype(np.float32)\n",
    "\n",
    "\n",
    "# HT = numpy_array_test.shape[0]\n",
    "# WT = numpy_array_test.shape[1]\n",
    "\n",
    "# HT2 = numpy_array_ans.shape[0]\n",
    "# WT2 = numpy_array_ans.shape[1]\n",
    "\n",
    "# numpy_array_test = numpy_array_test/100\n",
    "# numpy_array_ans = numpy_array_ans/(numpy_array_ans.size)\n",
    "    \n",
    "# print(numpy_array_test.shape)\n",
    "# # print(numpy_array_test)\n",
    "# # print(numpy_array_ans)\n",
    "\n",
    "# # print(H)\n",
    "# # print(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835409bc",
   "metadata": {},
   "source": [
    "## Tensorflow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1e109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# def create_model():\n",
    "#   return tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(28, 28), name='layers_input'),\n",
    "#     tf.keras.layers.Flatten(name='layers_flatten'),\n",
    "#     tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),\n",
    "#     tf.keras.layers.Dropout(0.2, name='layers_dropout'),\n",
    "#     tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')\n",
    "#   ])\n",
    "\n",
    "# model = create_model()\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# model.fit(x=x_train, \n",
    "#           y=y_train, \n",
    "#           epochs=5, \n",
    "#           validation_data=(x_test, y_test), \n",
    "#           callbacks=[tensorboard_callback])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021086ed",
   "metadata": {},
   "source": [
    "## My Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7b219b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T15:20:55.219341Z",
     "start_time": "2025-08-08T15:20:51.750889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0454 - loss: 80.0317 - val_accuracy: 0.2089 - val_loss: 34.1922\n",
      "Epoch 2/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0823 - loss: 30.7729 - val_accuracy: 0.1920 - val_loss: 26.9982\n",
      "Epoch 3/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1116 - loss: 22.4660 - val_accuracy: 0.2025 - val_loss: 23.0287\n",
      "Epoch 4/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1250 - loss: 14.9123 - val_accuracy: 0.2025 - val_loss: 19.5369\n",
      "Epoch 5/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1469 - loss: 11.8712 - val_accuracy: 0.2068 - val_loss: 17.7675\n",
      "Epoch 6/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1968 - loss: 10.3059 - val_accuracy: 0.2046 - val_loss: 16.3182\n",
      "Epoch 7/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2126 - loss: 8.0737 - val_accuracy: 0.2046 - val_loss: 14.8254\n",
      "Epoch 8/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2294 - loss: 7.2694 - val_accuracy: 0.2046 - val_loss: 12.5566\n",
      "Epoch 9/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2224 - loss: 6.3913 - val_accuracy: 0.2215 - val_loss: 11.3196\n",
      "Epoch 10/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2186 - loss: 5.4893 - val_accuracy: 0.2194 - val_loss: 10.5243\n",
      "Epoch 11/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2210 - loss: 5.1278 - val_accuracy: 0.2173 - val_loss: 9.7455\n",
      "Epoch 12/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2205 - loss: 4.8343 - val_accuracy: 0.2152 - val_loss: 9.6836\n",
      "Epoch 13/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2248 - loss: 4.5861 - val_accuracy: 0.2194 - val_loss: 9.9969\n",
      "Epoch 14/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2239 - loss: 4.6096 - val_accuracy: 0.2236 - val_loss: 9.9709\n",
      "Epoch 15/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2142 - loss: 4.5695 - val_accuracy: 0.2194 - val_loss: 9.9123\n",
      "Epoch 16/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2194 - loss: 4.4570 - val_accuracy: 0.2215 - val_loss: 10.8574\n",
      "Epoch 17/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2410 - loss: 4.3476 - val_accuracy: 0.2215 - val_loss: 10.9627\n",
      "Epoch 18/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2338 - loss: 4.3640 - val_accuracy: 0.2215 - val_loss: 10.9864\n",
      "Epoch 19/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2344 - loss: 4.2355 - val_accuracy: 0.2194 - val_loss: 11.6067\n",
      "Epoch 20/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2477 - loss: 4.1132 - val_accuracy: 0.2194 - val_loss: 11.3305\n",
      "Epoch 21/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2483 - loss: 4.1574 - val_accuracy: 0.2173 - val_loss: 11.2714\n",
      "Epoch 22/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2458 - loss: 4.0943 - val_accuracy: 0.2194 - val_loss: 11.6594\n",
      "Epoch 23/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2526 - loss: 4.0561 - val_accuracy: 0.2194 - val_loss: 11.8668\n",
      "Epoch 24/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2555 - loss: 3.9857 - val_accuracy: 0.2194 - val_loss: 11.9481\n",
      "Epoch 25/25\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2541 - loss: 3.9519 - val_accuracy: 0.2236 - val_loss: 12.3200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f0f2457990>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  # tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1000, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#   loss='mean_squared_error')\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# print(numpy_array[1,0])\n",
    "# model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
    "# model.evaluate(numpy_array_test, numpy_array_ans, verbose = 2)\n",
    "\n",
    "# python -m tensorboard.main --logdir=logs/fit\n",
    "\n",
    "# Use above to view data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164e765",
   "metadata": {},
   "source": [
    "## Single Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44aa691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 23)\n"
     ]
    }
   ],
   "source": [
    "# Generate some dummy input data\n",
    "# dummy_input = tf.random.normal((1, 1, 28))\n",
    "\n",
    "data = [2.700e-01, 1.600e-01, 1.600e-01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
    "0.000e+00, 0.000e+00, 3.450e+00, 2.005e+01, 5.810e-02, 1.300e-01, 4.300e-01,\n",
    "3.300e-01, 2.780e+00, 8.420e-02, 2.000e-02, 2.000e-02, 1.000e-02,\n",
    "1.500e-01, 3.220e+00, 3.553e+00, 4.300e-01]\n",
    "input = np.array(data)        \n",
    "reshaped_single_sample = input.reshape(1, -1) # Reshapes to (1, number_of_features)\n",
    "print(reshaped_single_sample.shape)\n",
    "\n",
    "# Get the output of the final layer\n",
    "# final_layer_output = model.predict(reshaped_single_sample)\n",
    "# print(reshaped_single_sample)\n",
    "# final_layer_output = final_layer_output*100\n",
    "# print(final_layer_output)\n",
    "\n",
    "# smallest = final_layer_output[0]\n",
    "# for val in final_layer_output:\n",
    "#     if val < smallest:\n",
    "#         smallest = val\n",
    "# print(smallest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
